{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "664b98e8",
   "metadata": {},
   "source": [
    "#### Day 26: DS Task 1 -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b47b60",
   "metadata": {},
   "source": [
    "### Data Scalling + SVM + Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e963a5a8",
   "metadata": {},
   "source": [
    "#### 1) Create a simple SVM model which takes renset50 features, image width and height, image's lbp features. You should first create a csv of all features and then train svm. Before training svm. You should scale your data too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a6e62f",
   "metadata": {},
   "source": [
    "#### 2) Explain in the code why did we scale the data before passing it to svm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d39142",
   "metadata": {},
   "source": [
    "#### 3) Script to Auto Create Accuracy Metrics Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338cacc6",
   "metadata": {},
   "source": [
    "#### 4) Create Model's Flask API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d3422",
   "metadata": {},
   "source": [
    "#### 5) Docker file of API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0055e3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "#with zipfile.ZipFile('archive (3).zip', 'r') as zip_ref:\n",
    "#    zip_ref.extractall('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9a47abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-08 11:34:50.047711: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-08 11:34:51.910516: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mrizwan/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-08 11:34:51.910753: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-08 11:35:13.619970: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mrizwan/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-08 11:35:13.621219: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/mrizwan/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-05-08 11:35:13.621315: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from skimage.feature import local_binary_pattern\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import os\n",
    "\n",
    "shoes_dir = '/Shoe vs Sandal vs Boot Dataset/Shoe'\n",
    "sandals_dir = '/Shoe vs Sandal vs Boot Dataset/Sandal'\n",
    "boots_dir = '/Shoe vs Sandal vs Boot Dataset/Boot'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b9668e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#code for creating csv\n",
    "\n",
    "\n",
    "\n",
    "def extract_resnet_features(img_path):\n",
    "# Load image and preprocess for ResNet50\n",
    "    img = load_img(img_path, target_size=(224, 224))\n",
    "    x = img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    # Extract features using ResNet50\n",
    "    model = ResNet50(weights='imagenet', include_top=False)\n",
    "    features = model.predict(x)\n",
    "    return features.flatten()\n",
    "def extract_lbp_features(img_path):\n",
    "# Load image and convert to grayscale\n",
    "    img = cv2.imread(img_path)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "# Extract LBP features\n",
    "    lbp = local_binary_pattern(gray, 8, 1)\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=np.arange(0, 257), range=(0, 256))\n",
    "    hist = hist.astype(\"float\")\n",
    "    hist /= (hist.sum() + 1e-7)\n",
    "    return hist\n",
    "def extract_image_dimensions(img_path):\n",
    "# Load image and get dimensions\n",
    "    img = cv2.imread(img_path)\n",
    "    height, width, _ = img.shape\n",
    "    return [height, width]\n",
    "resnet_features = []\n",
    "lbp_features = []\n",
    "image_dims = []\n",
    "labels = []\n",
    "for folder, label in [(shoes_dir, 0), (sandals_dir, 1), (boots_dir, 2)]:\n",
    "    for filename in os.listdir(folder):\n",
    "        if filename.endswith('.jpg'):\n",
    "# Extract features and dimensions for image\n",
    "            img_path = os.path.join(folder, filename)\n",
    "            resnet_feature = extract_resnet_features(img_path)\n",
    "            lbp_feature = extract_lbp_features(img_path)\n",
    "            image_dim = extract_image_dimensions(img_path)\n",
    "## Append features and dimensions to lists\n",
    "            resnet_features.append(resnet_feature)\n",
    "            lbp_features.append(lbp_feature)\n",
    "            image_dims.append(image_dim)\n",
    "            labels.append(label)\n",
    "\n",
    "#Convert lists to arrays\n",
    "resnet_features = np.array(resnet_features)\n",
    "lbp_features = np.array(lbp_features)\n",
    "image_dims = np.array(image_dims)\n",
    "labels = np.array(labels)\n",
    "\n",
    "#Create dataframes from arrays\n",
    "resnet_df = pd.DataFrame(resnet_features)\n",
    "lbp_df = pd.DataFrame(lbp_features)\n",
    "dims_df = pd.DataFrame(image_dims)\n",
    "labels_df = pd.DataFrame(labels, columns=['label'])\n",
    "\n",
    "#Concatenate dataframes\n",
    "features_df = pd.concat([resnet_df, lbp_df, dims_df], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92222feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save dataframes to CSV files\n",
    "features_df.to_csv('features.csv', index=False)\n",
    "labels_df.to_csv('labels.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db266f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

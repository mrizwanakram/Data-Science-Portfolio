{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:17:51.305891Z",
     "iopub.status.busy": "2023-03-31T07:17:51.305562Z",
     "iopub.status.idle": "2023-03-31T07:18:04.808305Z",
     "shell.execute_reply": "2023-03-31T07:18:04.807213Z",
     "shell.execute_reply.started": "2023-03-31T07:17:51.305851Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "\n",
    "\n",
    "# importing the zipfile module\n",
    "from zipfile import ZipFile\n",
    "  \n",
    "# loading the temp.zip and creating a zip object\n",
    "with ZipFile(\"Bert_base_uncased.zip\", 'r') as zObject:\n",
    "  \n",
    "    # Extracting all the members of the zip \n",
    "    # into a specific location.\n",
    "    zObject.extractall(\n",
    "        path=\"Bert_base_uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:04.810579Z",
     "iopub.status.busy": "2023-03-31T07:18:04.810260Z",
     "iopub.status.idle": "2023-03-31T07:18:07.159764Z",
     "shell.execute_reply": "2023-03-31T07:18:07.158721Z",
     "shell.execute_reply.started": "2023-03-31T07:18:04.810548Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from transformers import BertForTokenClassification, BertTokenizerFast\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "\n",
    "# from seqeval.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo install seqeval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.161991Z",
     "iopub.status.busy": "2023-03-31T07:18:07.161569Z",
     "iopub.status.idle": "2023-03-31T07:18:07.273852Z",
     "shell.execute_reply": "2023-03-31T07:18:07.272815Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.161935Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 500\n",
    "EPOCHS = 4\n",
    "MODEL_PATH = 'bert-base-uncased'\n",
    "TOKENIZER = BertTokenizerFast('Bert_base_uncased/vocab.txt', lowercase=True)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.276012Z",
     "iopub.status.busy": "2023-03-31T07:18:07.275618Z",
     "iopub.status.idle": "2023-03-31T07:18:07.690486Z",
     "shell.execute_reply": "2023-03-31T07:18:07.689585Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.275957Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_json('Entity Recognition in Resumes.json/Entity Recognition in Resumes.json', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.694259Z",
     "iopub.status.busy": "2023-03-31T07:18:07.693899Z",
     "iopub.status.idle": "2023-03-31T07:18:07.769432Z",
     "shell.execute_reply": "2023-03-31T07:18:07.768494Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.694224Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>annotation</th>\n",
       "      <th>extras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Abhishek Jha\\nApplication Development Associat...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 12...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afreen Jamadar\\nActive member of IIIT Committe...</td>\n",
       "      <td>[{'label': ['Email Address'], 'points': [{'sta...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Akhil Yadav Polemaina\\nHyderabad, Telangana - ...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 37...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alok Khandai\\nOperational Analyst (SQL DBA) En...</td>\n",
       "      <td>[{'label': ['Skills'], 'points': [{'start': 80...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...</td>\n",
       "      <td>[{'label': ['Degree'], 'points': [{'start': 20...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  \\\n",
       "0  Abhishek Jha\\nApplication Development Associat...   \n",
       "1  Afreen Jamadar\\nActive member of IIIT Committe...   \n",
       "2  Akhil Yadav Polemaina\\nHyderabad, Telangana - ...   \n",
       "3  Alok Khandai\\nOperational Analyst (SQL DBA) En...   \n",
       "4  Ananya Chavan\\nlecturer - oracle tutorials\\n\\n...   \n",
       "\n",
       "                                          annotation  extras  \n",
       "0  [{'label': ['Skills'], 'points': [{'start': 12...     NaN  \n",
       "1  [{'label': ['Email Address'], 'points': [{'sta...     NaN  \n",
       "2  [{'label': ['Skills'], 'points': [{'start': 37...     NaN  \n",
       "3  [{'label': ['Skills'], 'points': [{'start': 80...     NaN  \n",
       "4  [{'label': ['Degree'], 'points': [{'start': 20...     NaN  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.773147Z",
     "iopub.status.busy": "2023-03-31T07:18:07.772761Z",
     "iopub.status.idle": "2023-03-31T07:18:07.791119Z",
     "shell.execute_reply": "2023-03-31T07:18:07.790052Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.773116Z"
    }
   },
   "outputs": [],
   "source": [
    "def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n",
    "    try:\n",
    "        training_data = []\n",
    "        lines=[]\n",
    "        with open(dataturks_JSON_FilePath, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            data = json.loads(line)\n",
    "            text = data['content'].replace(\"\\n\", \" \")\n",
    "            entities = []\n",
    "            data_annotations = data['annotation']\n",
    "            if data_annotations is not None:\n",
    "                for annotation in data_annotations:\n",
    "                    #only a single point in text annotation.\n",
    "                    point = annotation['points'][0]\n",
    "                    labels = annotation['label']\n",
    "                    # handle both list of labels or a single label.\n",
    "                    if not isinstance(labels, list):\n",
    "                        labels = [labels]\n",
    "\n",
    "                    for label in labels:\n",
    "                        point_start = point['start']\n",
    "                        point_end = point['end']\n",
    "                        point_text = point['text']\n",
    "                        \n",
    "                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n",
    "                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n",
    "                        if lstrip_diff != 0:\n",
    "                            point_start = point_start + lstrip_diff\n",
    "                        if rstrip_diff != 0:\n",
    "                            point_end = point_end - rstrip_diff\n",
    "                        entities.append((point_start, point_end + 1 , label))\n",
    "            training_data.append((text, {\"entities\" : entities}))\n",
    "        return training_data\n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n",
    "        return None\n",
    "\n",
    "def trim_entity_spans(data: list) -> list:\n",
    "    \"\"\"Removes leading and trailing white spaces from entity spans.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to be cleaned in spaCy JSON format.\n",
    "\n",
    "    Returns:\n",
    "        list: The cleaned data.\n",
    "    \"\"\"\n",
    "    invalid_span_tokens = re.compile(r'\\s')\n",
    "\n",
    "    cleaned_data = []\n",
    "    for text, annotations in data:\n",
    "        entities = annotations['entities']\n",
    "        valid_entities = []\n",
    "        for start, end, label in entities:\n",
    "            valid_start = start\n",
    "            valid_end = end\n",
    "            while valid_start < len(text) and invalid_span_tokens.match(\n",
    "                    text[valid_start]):\n",
    "                valid_start += 1\n",
    "            while valid_end > 1 and invalid_span_tokens.match(\n",
    "                    text[valid_end - 1]):\n",
    "                valid_end -= 1\n",
    "            valid_entities.append([valid_start, valid_end, label])\n",
    "        cleaned_data.append([text, {'entities': valid_entities}])\n",
    "    return cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.793755Z",
     "iopub.status.busy": "2023-03-31T07:18:07.792998Z",
     "iopub.status.idle": "2023-03-31T07:18:07.828168Z",
     "shell.execute_reply": "2023-03-31T07:18:07.827478Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.793715Z"
    }
   },
   "outputs": [],
   "source": [
    "data = trim_entity_spans(convert_dataturks_to_spacy('Entity Recognition in Resumes.json/Entity Recognition in Resumes.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.829967Z",
     "iopub.status.busy": "2023-03-31T07:18:07.829433Z",
     "iopub.status.idle": "2023-03-31T07:18:07.837977Z",
     "shell.execute_reply": "2023-03-31T07:18:07.837108Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.829930Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_label(offset, labels):\n",
    "    if offset[0] == 0 and offset[1] == 0:\n",
    "        return 'O'\n",
    "    for label in labels:\n",
    "        if offset[1] >= label[0] and offset[0] <= label[1]:\n",
    "            return label[2]\n",
    "    return 'O'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.840169Z",
     "iopub.status.busy": "2023-03-31T07:18:07.839678Z",
     "iopub.status.idle": "2023-03-31T07:18:07.847631Z",
     "shell.execute_reply": "2023-03-31T07:18:07.846464Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.840129Z"
    }
   },
   "outputs": [],
   "source": [
    "tags_vals = [\"UNKNOWN\", \"O\", \"Name\", \"Degree\",\"Skills\",\"College Name\",\"Email Address\",\"Designation\",\"Companies worked at\",\"Graduation Year\",\"Years of Experience\",\"Location\"]\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "idx2tag = {i:t for i, t in enumerate(tags_vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.849977Z",
     "iopub.status.busy": "2023-03-31T07:18:07.849284Z",
     "iopub.status.idle": "2023-03-31T07:18:07.860611Z",
     "shell.execute_reply": "2023-03-31T07:18:07.860008Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.849940Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_resume(data, tokenizer, tag2idx, max_len, is_test=False):\n",
    "    tok = tokenizer.encode_plus(data[0], max_length=max_len, return_offsets_mapping=True)\n",
    "    curr_sent = {'orig_labels':[], 'labels': []}\n",
    "    \n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "    \n",
    "    if not is_test:\n",
    "        labels = data[1]['entities']\n",
    "        labels.reverse()\n",
    "        for off in tok['offset_mapping']:\n",
    "            label = get_label(off, labels)\n",
    "            curr_sent['orig_labels'].append(label)\n",
    "            curr_sent['labels'].append(tag2idx[label])\n",
    "        curr_sent['labels'] = curr_sent['labels'] + ([0] * padding_length)\n",
    "    \n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + ([0] * padding_length)\n",
    "    return curr_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.862988Z",
     "iopub.status.busy": "2023-03-31T07:18:07.862341Z",
     "iopub.status.idle": "2023-03-31T07:18:07.874098Z",
     "shell.execute_reply": "2023-03-31T07:18:07.873163Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.862950Z"
    }
   },
   "outputs": [],
   "source": [
    "class ResumeDataset(Dataset):\n",
    "    def __init__(self, resume, tokenizer, tag2idx, max_len, is_test=False):\n",
    "        self.resume = resume\n",
    "        self.tokenizer = tokenizer\n",
    "        self.is_test = is_test\n",
    "        self.tag2idx = tag2idx\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.resume)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        data = process_resume(self.resume[idx], self.tokenizer, self.tag2idx, self.max_len, self.is_test)\n",
    "        return {\n",
    "            'input_ids': torch.tensor(data['input_ids'], dtype=torch.long),\n",
    "            'token_type_ids': torch.tensor(data['token_type_ids'], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor(data['attention_mask'], dtype=torch.long),\n",
    "            'labels': torch.tensor(data['labels'], dtype=torch.long),\n",
    "            'orig_label': data['orig_labels']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.876228Z",
     "iopub.status.busy": "2023-03-31T07:18:07.875798Z",
     "iopub.status.idle": "2023-03-31T07:18:07.883438Z",
     "shell.execute_reply": "2023-03-31T07:18:07.882282Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.876190Z"
    }
   },
   "outputs": [],
   "source": [
    "total = len(data)\n",
    "train_data, val_data = data[:180], data[180:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.885575Z",
     "iopub.status.busy": "2023-03-31T07:18:07.885236Z",
     "iopub.status.idle": "2023-03-31T07:18:07.895489Z",
     "shell.execute_reply": "2023-03-31T07:18:07.894747Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.885540Z"
    }
   },
   "outputs": [],
   "source": [
    "train_d = ResumeDataset(train_data, TOKENIZER, tag2idx, MAX_LEN)\n",
    "val_d = ResumeDataset(val_data, TOKENIZER, tag2idx, MAX_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.897468Z",
     "iopub.status.busy": "2023-03-31T07:18:07.897081Z",
     "iopub.status.idle": "2023-03-31T07:18:07.904399Z",
     "shell.execute_reply": "2023-03-31T07:18:07.903290Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.897428Z"
    }
   },
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_d)\n",
    "train_dl = DataLoader(train_d, sampler=train_sampler, batch_size=8)\n",
    "\n",
    "val_dl = DataLoader(val_d, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.906833Z",
     "iopub.status.busy": "2023-03-31T07:18:07.906403Z",
     "iopub.status.idle": "2023-03-31T07:18:07.916168Z",
     "shell.execute_reply": "2023-03-31T07:18:07.915228Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.906756Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_hyperparameters(model, ff):\n",
    "\n",
    "    # ff: full_finetuning\n",
    "    if ff:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.01,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters())\n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.918664Z",
     "iopub.status.busy": "2023-03-31T07:18:07.918306Z",
     "iopub.status.idle": "2023-03-31T07:18:07.927093Z",
     "shell.execute_reply": "2023-03-31T07:18:07.926103Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.918630Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_special_tokens(tokenizer, tag2idx):\n",
    "    vocab = tokenizer.get_vocab()\n",
    "    pad_tok = vocab[\"[PAD]\"]\n",
    "    sep_tok = vocab[\"[SEP]\"]\n",
    "    cls_tok = vocab[\"[CLS]\"]\n",
    "    o_lab = tag2idx[\"O\"]\n",
    "\n",
    "    return pad_tok, sep_tok, cls_tok, o_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.930077Z",
     "iopub.status.busy": "2023-03-31T07:18:07.929646Z",
     "iopub.status.idle": "2023-03-31T07:18:07.937038Z",
     "shell.execute_reply": "2023-03-31T07:18:07.935912Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.930039Z"
    }
   },
   "outputs": [],
   "source": [
    "def annot_confusion_matrix(valid_tags, pred_tags):\n",
    "\n",
    "    \"\"\"\n",
    "    Create an annotated confusion matrix by adding label\n",
    "    annotations and formatting to sklearn's `confusion_matrix`.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create header from unique tags\n",
    "    header = sorted(list(set(valid_tags + pred_tags)))\n",
    "\n",
    "    # Calculate the actual confusion matrix\n",
    "    matrix = confusion_matrix(valid_tags, pred_tags, labels=header)\n",
    "\n",
    "    # Final formatting touches for the string output\n",
    "    mat_formatted = [header[i] + \"\\t\\t\\t\" + str(row) for i, row in enumerate(matrix)]\n",
    "    content = \"\\t\" + \" \".join(header) + \"\\n\" + \"\\n\".join(mat_formatted)\n",
    "\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.938965Z",
     "iopub.status.busy": "2023-03-31T07:18:07.938696Z",
     "iopub.status.idle": "2023-03-31T07:18:07.947953Z",
     "shell.execute_reply": "2023-03-31T07:18:07.947240Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.938941Z"
    }
   },
   "outputs": [],
   "source": [
    "def flat_accuracy(valid_tags, pred_tags):\n",
    "    return (np.array(valid_tags) == np.array(pred_tags)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:07.950029Z",
     "iopub.status.busy": "2023-03-31T07:18:07.949631Z",
     "iopub.status.idle": "2023-03-31T07:18:22.271344Z",
     "shell.execute_reply": "2023-03-31T07:18:22.270441Z",
     "shell.execute_reply.started": "2023-03-31T07:18:07.949973Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Downloading pytorch_model.bin:   0%|                                                         | 0.00/440M [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:   2%|█▏                                               | 10.5M/440M [00:38<26:35, 269kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:   2%|█▏                                               | 10.5M/440M [00:52<26:35, 269kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:   5%|██▎                                              | 21.0M/440M [01:18<26:01, 269kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:   5%|██▎                                              | 21.0M/440M [01:32<26:01, 269kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:   7%|███▍                                             | 31.5M/440M [02:05<28:00, 243kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:   7%|███▍                                             | 31.5M/440M [02:22<28:00, 243kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  10%|████▋                                            | 41.9M/440M [02:42<25:43, 258kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  10%|████▋                                            | 41.9M/440M [03:02<25:43, 258kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  12%|█████▊                                           | 52.4M/440M [03:19<24:07, 268kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  12%|█████▊                                           | 52.4M/440M [03:32<24:07, 268kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  14%|██████▉                                          | 62.9M/440M [03:44<20:33, 306kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  14%|██████▉                                          | 62.9M/440M [04:02<20:33, 306kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  17%|████████▏                                        | 73.4M/440M [04:16<19:41, 311kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  17%|████████▏                                        | 73.4M/440M [04:32<19:41, 311kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  19%|█████████▎                                       | 83.9M/440M [04:39<17:11, 346kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  21%|██████████▍                                      | 94.4M/440M [04:51<13:25, 429kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  21%|██████████▍                                      | 94.4M/440M [05:02<13:25, 429kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  24%|███████████▉                                      | 105M/440M [05:03<10:58, 510kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  26%|█████████████                                     | 115M/440M [05:21<10:12, 531kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  26%|█████████████                                     | 115M/440M [05:32<10:12, 531kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  29%|██████████████▎                                   | 126M/440M [06:06<13:43, 382kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  31%|███████████████▍                                  | 136M/440M [06:17<10:52, 466kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  31%|███████████████▍                                  | 136M/440M [06:33<10:52, 466kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  33%|████████████████▋                                 | 147M/440M [06:43<11:04, 442kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  36%|█████████████████▊                                | 157M/440M [06:53<08:49, 535kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  36%|█████████████████▊                                | 157M/440M [07:13<08:49, 535kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  38%|███████████████████                               | 168M/440M [07:13<08:30, 534kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  40%|████████████████████▏                             | 178M/440M [07:30<07:47, 560kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  40%|████████████████████▏                             | 178M/440M [07:43<07:47, 560kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  43%|█████████████████████▍                            | 189M/440M [07:51<07:47, 539kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  43%|█████████████████████▍                            | 189M/440M [08:03<07:47, 539kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  45%|██████████████████████▌                           | 199M/440M [08:07<07:03, 570kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  48%|███████████████████████▊                          | 210M/440M [08:17<05:53, 652kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  50%|████████████████████████▉                         | 220M/440M [08:32<05:31, 665kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  50%|████████████████████████▉                         | 220M/440M [08:43<05:31, 665kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  52%|██████████████████████████▏                       | 231M/440M [08:50<05:24, 647kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  55%|███████████████████████████▍                      | 241M/440M [08:58<04:25, 750kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  57%|████████████████████████████▌                     | 252M/440M [09:11<04:06, 766kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  57%|████████████████████████████▌                     | 252M/440M [09:23<04:06, 766kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  60%|█████████████████████████████▊                    | 262M/440M [09:32<04:26, 669kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  60%|█████████████████████████████▊                    | 262M/440M [09:43<04:26, 669kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  62%|██████████████████████████████▉                   | 273M/440M [09:44<03:53, 719kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  64%|████████████████████████████████▏                 | 283M/440M [09:58<03:35, 731kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  64%|████████████████████████████████▏                 | 283M/440M [10:13<03:35, 731kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  67%|█████████████████████████████████▎                | 294M/440M [10:16<03:35, 680kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  69%|██████████████████████████████████▌               | 304M/440M [10:32<03:26, 660kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  69%|██████████████████████████████████▌               | 304M/440M [10:43<03:26, 660kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  71%|███████████████████████████████████▋              | 315M/440M [10:47<03:04, 684kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  71%|███████████████████████████████████▋              | 315M/440M [11:03<03:04, 684kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  74%|████████████████████████████████████▉             | 325M/440M [11:10<03:14, 593kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  74%|████████████████████████████████████▉             | 325M/440M [11:23<03:14, 593kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  76%|██████████████████████████████████████            | 336M/440M [11:43<03:44, 467kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  79%|███████████████████████████████████████▎          | 346M/440M [11:52<02:44, 574kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  79%|███████████████████████████████████████▎          | 346M/440M [12:03<02:44, 574kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  81%|████████████████████████████████████████▍         | 357M/440M [12:04<02:12, 636kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  81%|████████████████████████████████████████▍         | 357M/440M [12:23<02:12, 636kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  83%|█████████████████████████████████████████▋        | 367M/440M [12:24<02:01, 604kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  83%|█████████████████████████████████████████▋        | 367M/440M [12:43<02:01, 604kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  86%|██████████████████████████████████████████▊       | 377M/440M [12:49<01:58, 531kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  86%|██████████████████████████████████████████▊       | 377M/440M [13:03<01:58, 531kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  88%|████████████████████████████████████████████      | 388M/440M [13:03<01:31, 577kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  90%|█████████████████████████████████████████████▏    | 398M/440M [13:20<01:10, 596kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  90%|█████████████████████████████████████████████▏    | 398M/440M [13:33<01:10, 596kB/s]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin:  93%|██████████████████████████████████████████████▍   | 409M/440M [13:38<00:53, 585kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  93%|██████████████████████████████████████████████▍   | 409M/440M [13:53<00:53, 585kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  95%|███████████████████████████████████████████████▌  | 419M/440M [14:46<01:06, 318kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  95%|███████████████████████████████████████████████▌  | 419M/440M [15:03<01:06, 318kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  98%|████████████████████████████████████████████████▊ | 430M/440M [15:51<00:42, 247kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin:  98%|████████████████████████████████████████████████▊ | 430M/440M [16:03<00:42, 247kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin: 100%|█████████████████████████████████████████████████▉| 440M/440M [16:44<00:00, 230kB/s]\u001b[A\u001b[A\n",
      "\n",
      "Downloading pytorch_model.bin: 100%|██████████████████████████████████████████████████| 440M/440M [16:44<00:00, 439kB/s]\u001b[A\u001b[A\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForTokenClassification.from_pretrained(MODEL_PATH, num_labels=len(tag2idx))\n",
    "model.to(DEVICE);\n",
    "optimizer_grouped_parameters = get_hyperparameters(model, True)\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:22.273218Z",
     "iopub.status.busy": "2023-03-31T07:18:22.272799Z",
     "iopub.status.idle": "2023-03-31T07:18:22.279669Z",
     "shell.execute_reply": "2023-03-31T07:18:22.278699Z",
     "shell.execute_reply.started": "2023-03-31T07:18:22.273169Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_GRAD_NORM = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:22.282114Z",
     "iopub.status.busy": "2023-03-31T07:18:22.281691Z",
     "iopub.status.idle": "2023-03-31T07:18:22.313713Z",
     "shell.execute_reply": "2023-03-31T07:18:22.312735Z",
     "shell.execute_reply.started": "2023-03-31T07:18:22.282073Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_save_model(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    optimizer,\n",
    "    epochs,\n",
    "    idx2tag,\n",
    "    tag2idx,\n",
    "    max_grad_norm,\n",
    "    device,\n",
    "    train_dataloader,\n",
    "    valid_dataloader\n",
    "):\n",
    "\n",
    "    pad_tok, sep_tok, cls_tok, o_lab = get_special_tokens(tokenizer, tag2idx)\n",
    "    \n",
    "    epoch = 0\n",
    "    val_acc=[]\n",
    "    ep=[]\n",
    "    val_loss=[]\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        epoch += 1\n",
    "\n",
    "        # Training loop\n",
    "        print(\"Starting training loop.\")\n",
    "        model.train()\n",
    "        tr_loss, tr_accuracy = 0, 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        tr_preds, tr_labels = [], []\n",
    "\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Add batch to gpu\n",
    "            \n",
    "            # batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "            b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(\n",
    "                b_input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=b_input_mask,\n",
    "                labels=b_labels,\n",
    "            )\n",
    "            loss, tr_logits = outputs[:2]\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Compute train loss\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "            # Subset out unwanted predictions on CLS/PAD/SEP tokens\n",
    "            preds_mask = (\n",
    "                (b_input_ids != cls_tok)\n",
    "                & (b_input_ids != pad_tok)\n",
    "                & (b_input_ids != sep_tok)\n",
    "            )\n",
    "\n",
    "            tr_logits = tr_logits.cpu().detach().numpy()\n",
    "            tr_label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
    "            preds_mask = preds_mask.cpu().detach().numpy()\n",
    "            tr_batch_preds = np.argmax(tr_logits[preds_mask.squeeze()], axis=1)\n",
    "            tr_batch_labels = tr_label_ids.to(\"cpu\").numpy()\n",
    "            tr_preds.extend(tr_batch_preds)\n",
    "            tr_labels.extend(tr_batch_labels)\n",
    "\n",
    "            # Compute training accuracy\n",
    "            tmp_tr_accuracy = flat_accuracy(tr_batch_labels, tr_batch_preds)\n",
    "            tr_accuracy += tmp_tr_accuracy\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                parameters=model.parameters(), max_norm=max_grad_norm\n",
    "            )\n",
    "\n",
    "            # Update parameters\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "        tr_loss = tr_loss / nb_tr_steps\n",
    "        tr_accuracy = tr_accuracy / nb_tr_steps\n",
    "\n",
    "        # Print training loss and accuracy per epoch\n",
    "        print(f\"Train loss: {tr_loss}\")\n",
    "        print(f\"Train accuracy: {tr_accuracy}\")\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Validation loop\n",
    "        \"\"\" \n",
    "        print(\"Starting validation loop.\")\n",
    "\n",
    "        model.eval()\n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        predictions, true_labels = [], []\n",
    "\n",
    "        for batch in valid_dataloader:\n",
    "\n",
    "            b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "            b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(\n",
    "                    b_input_ids,\n",
    "                    token_type_ids=None,\n",
    "                    attention_mask=b_input_mask,\n",
    "                    labels=b_labels,\n",
    "                )\n",
    "                tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            # Subset out unwanted predictions on CLS/PAD/SEP tokens\n",
    "            preds_mask = (\n",
    "                (b_input_ids != cls_tok)\n",
    "                & (b_input_ids != pad_tok)\n",
    "                & (b_input_ids != sep_tok)\n",
    "            )\n",
    "\n",
    "            logits = logits.cpu().detach().numpy()\n",
    "            label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n",
    "            preds_mask = preds_mask.cpu().detach().numpy()\n",
    "            val_batch_preds = np.argmax(logits[preds_mask.squeeze()], axis=1)\n",
    "            val_batch_labels = label_ids.to(\"cpu\").numpy()\n",
    "            predictions.extend(val_batch_preds)\n",
    "            true_labels.extend(val_batch_labels)\n",
    "\n",
    "            tmp_eval_accuracy = flat_accuracy(val_batch_labels, val_batch_preds)\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "            nb_eval_examples += b_input_ids.size(0)\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        # Evaluate loss, acc, conf. matrix, and class. report on devset\n",
    "        pred_tags = [idx2tag[i] for i in predictions]\n",
    "        valid_tags = [idx2tag[i] for i in true_labels]\n",
    "        cl_report = classification_report([valid_tags], [pred_tags])\n",
    "        conf_mat = annot_confusion_matrix(valid_tags, pred_tags)\n",
    "        eval_loss = eval_loss / nb_eval_steps\n",
    "        eval_accuracy = eval_accuracy / nb_eval_steps\n",
    "\n",
    "        # Report metrics\n",
    "        print(f\"Validation loss: {eval_loss}\")\n",
    "        print(f\"Validation Accuracy: {eval_accuracy}\")\n",
    "        print(f\"Classification Report:\\n {cl_report}\")\n",
    "        print(f\"Confusion Matrix:\\n {conf_mat}\")\n",
    "        val_acc.append(eval_accuracy)\n",
    "        ep.append(epoch)\n",
    "        val_loss.append(eval_loss)\n",
    "    \n",
    "    plt.plot(ep, val_acc, 'g', label='Validation accuracy')\n",
    "    #plt.plot(ep, val_loss, 'b', label='Validation loss')\n",
    "    plt.title('Validation acuuracy ')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:18:22.317113Z",
     "iopub.status.busy": "2023-03-31T07:18:22.316741Z",
     "iopub.status.idle": "2023-03-31T07:19:09.983654Z",
     "shell.execute_reply": "2023-03-31T07:19:09.982724Z",
     "shell.execute_reply.started": "2023-03-31T07:18:22.317087Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Epoch:   0%|                                                                                      | 0/4 [00:00<?, ?it/s]\u001b[A\u001b[ATruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Epoch:   0%|                                                                                      | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "each element in list of batch should be of equal size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_and_save_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43midx2tag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtag2idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMAX_GRAD_NORM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dl\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 30\u001b[0m, in \u001b[0;36mtrain_and_save_model\u001b[0;34m(model, tokenizer, optimizer, epochs, idx2tag, tag2idx, max_grad_norm, device, train_dataloader, valid_dataloader)\u001b[0m\n\u001b[1;32m     27\u001b[0m nb_tr_examples, nb_tr_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     28\u001b[0m tr_preds, tr_labels \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;66;03m# Add batch to gpu\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# batch = tuple(t.to(device) for t in batch)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     b_input_ids, b_input_mask, b_labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m], batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     35\u001b[0m     b_input_ids, b_input_mask, b_labels \u001b[38;5;241m=\u001b[39m b_input_ids\u001b[38;5;241m.\u001b[39mto(device), b_input_mask\u001b[38;5;241m.\u001b[39mto(device), b_labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:128\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:128\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mMapping):\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 128\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m elem_type({key: \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem})\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m         \u001b[38;5;66;03m# The mapping type may not support `__init__(iterable)`.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {key: collate([d[key] \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m batch], collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m elem}\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/collate.py:139\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    137\u001b[0m elem_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mnext\u001b[39m(it))\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(elem) \u001b[38;5;241m==\u001b[39m elem_size \u001b[38;5;28;01mfor\u001b[39;00m elem \u001b[38;5;129;01min\u001b[39;00m it):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meach element in list of batch should be of equal size\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    140\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: each element in list of batch should be of equal size"
     ]
    }
   ],
   "source": [
    "train_and_save_model(\n",
    "    model, \n",
    "    TOKENIZER, \n",
    "    optimizer, \n",
    "    EPOCHS, \n",
    "    idx2tag, \n",
    "    tag2idx, \n",
    "    MAX_GRAD_NORM, \n",
    "    DEVICE, \n",
    "    train_dl, \n",
    "    val_dl\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:19:09.985587Z",
     "iopub.status.busy": "2023-03-31T07:19:09.985074Z",
     "iopub.status.idle": "2023-03-31T07:19:11.469935Z",
     "shell.execute_reply": "2023-03-31T07:19:11.469035Z",
     "shell.execute_reply.started": "2023-03-31T07:19:09.985545Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(\n",
    "    {\n",
    "        \"epoch\": EPOCHS,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    },\n",
    "    'model_e10.tar',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:19:11.471742Z",
     "iopub.status.busy": "2023-03-31T07:19:11.471396Z",
     "iopub.status.idle": "2023-03-31T07:19:15.194385Z",
     "shell.execute_reply": "2023-03-31T07:19:15.192950Z",
     "shell.execute_reply.started": "2023-03-31T07:19:11.471706Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 500\n",
    "EPOCHS = 6\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "MODEL_PATH = '../input/bert-base-uncased'\n",
    "STATE_DICT = torch.load('../working/model_e10.tar', map_location=DEVICE)\n",
    "TOKENIZER = BertTokenizerFast('../input/bert-base-uncased/vocab.txt', lowercase=True)\n",
    "MODEL = BertForTokenClassification.from_pretrained(MODEL_PATH, state_dict=STATE_DICT['model_state_dict'], num_labels=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:19:15.205270Z",
     "iopub.status.busy": "2023-03-31T07:19:15.203191Z",
     "iopub.status.idle": "2023-03-31T07:19:15.220145Z",
     "shell.execute_reply": "2023-03-31T07:19:15.219131Z",
     "shell.execute_reply.started": "2023-03-31T07:19:15.205213Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_resume2(text, tokenizer, max_len):\n",
    "    tok = tokenizer.encode_plus(text, max_length=max_len, return_offsets_mapping=True)\n",
    "    \n",
    "    curr_sent = dict()\n",
    "    \n",
    "    padding_length = max_len - len(tok['input_ids'])\n",
    "        \n",
    "    curr_sent['input_ids'] = tok['input_ids'] + ([0] * padding_length)\n",
    "    curr_sent['token_type_ids'] = tok['token_type_ids'] + ([0] * padding_length)\n",
    "    curr_sent['attention_mask'] = tok['attention_mask'] + ([0] * padding_length)\n",
    "    \n",
    "    final_data = {\n",
    "        'input_ids': torch.tensor(curr_sent['input_ids'], dtype=torch.long),\n",
    "        'token_type_ids': torch.tensor(curr_sent['token_type_ids'], dtype=torch.long),\n",
    "        'attention_mask': torch.tensor(curr_sent['attention_mask'], dtype=torch.long),\n",
    "        'offset_mapping': tok['offset_mapping']\n",
    "    }\n",
    "    \n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:19:15.222417Z",
     "iopub.status.busy": "2023-03-31T07:19:15.221876Z",
     "iopub.status.idle": "2023-03-31T07:19:15.256948Z",
     "shell.execute_reply": "2023-03-31T07:19:15.253704Z",
     "shell.execute_reply.started": "2023-03-31T07:19:15.222368Z"
    }
   },
   "outputs": [],
   "source": [
    "tags_vals = [\"UNKNOWN\", \"O\", \"Name\", \"Degree\",\"Skills\",\"College Name\",\"Email Address\",\"Designation\",\"Companies worked at\",\"Graduation Year\",\"Years of Experience\",\"Location\"]\n",
    "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "idx2tag = {i:t for i, t in enumerate(tags_vals)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:19:15.264625Z",
     "iopub.status.busy": "2023-03-31T07:19:15.262300Z",
     "iopub.status.idle": "2023-03-31T07:19:15.280515Z",
     "shell.execute_reply": "2023-03-31T07:19:15.279344Z",
     "shell.execute_reply.started": "2023-03-31T07:19:15.264569Z"
    }
   },
   "outputs": [],
   "source": [
    "model = MODEL\n",
    "model.to(DEVICE);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:19:15.288978Z",
     "iopub.status.busy": "2023-03-31T07:19:15.285867Z",
     "iopub.status.idle": "2023-03-31T07:19:15.332817Z",
     "shell.execute_reply": "2023-03-31T07:19:15.331553Z",
     "shell.execute_reply.started": "2023-03-31T07:19:15.288922Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, idx2tag, tag2idx, device, test_resume):\n",
    "    model.eval()\n",
    "    data = process_resume2(test_resume, tokenizer, MAX_LEN)\n",
    "    input_ids, input_mask = data['input_ids'].unsqueeze(0), data['attention_mask'].unsqueeze(0)\n",
    "    labels = torch.tensor([1] * input_ids.size(0), dtype=torch.long).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=input_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "    \n",
    "    logits = logits.cpu().detach().numpy()\n",
    "    label_ids = np.argmax(logits, axis=2)\n",
    "    \n",
    "    entities = []\n",
    "    for label_id, offset in zip(label_ids[0], data['offset_mapping']):\n",
    "        curr_id = idx2tag[label_id]\n",
    "        curr_start = offset[0]\n",
    "        curr_end = offset[1]\n",
    "        if curr_id != 'O':\n",
    "            if len(entities) > 0 and entities[-1]['entity'] == curr_id and curr_start - entities[-1]['end'] in [0, 1]:\n",
    "                entities[-1]['end'] = curr_end\n",
    "            else:\n",
    "                entities.append({'entity': curr_id, 'start': curr_start, 'end':curr_end})\n",
    "    for ent in entities:\n",
    "        ent['text'] = test_resume[ent['start']:ent['end']]\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:19:15.342163Z",
     "iopub.status.busy": "2023-03-31T07:19:15.338565Z",
     "iopub.status.idle": "2023-03-31T07:19:26.394012Z",
     "shell.execute_reply": "2023-03-31T07:19:26.393090Z",
     "shell.execute_reply.started": "2023-03-31T07:19:15.342106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdfminer.six\n",
      "  Downloading pdfminer.six-20221105-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 9.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting cryptography>=36.0.0\n",
      "  Downloading cryptography-40.0.1-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 47.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from pdfminer.six) (3.7.4.1)\n",
      "Collecting charset-normalizer>=2.0.0\n",
      "  Downloading charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (171 kB)\n",
      "\u001b[K     |████████████████████████████████| 171 kB 54.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /opt/conda/lib/python3.7/site-packages (from cryptography>=36.0.0->pdfminer.six) (1.14.0)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.20)\n",
      "Installing collected packages: cryptography, charset-normalizer, pdfminer.six\n",
      "  Attempting uninstall: cryptography\n",
      "    Found existing installation: cryptography 2.8\n",
      "    Uninstalling cryptography-2.8:\n",
      "      Successfully uninstalled cryptography-2.8\n",
      "Successfully installed charset-normalizer-3.1.0 cryptography-40.0.1 pdfminer.six-20221105\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:19:26.398465Z",
     "iopub.status.busy": "2023-03-31T07:19:26.398151Z",
     "iopub.status.idle": "2023-03-31T07:19:26.443643Z",
     "shell.execute_reply": "2023-03-31T07:19:26.442683Z",
     "shell.execute_reply.started": "2023-03-31T07:19:26.398432Z"
    }
   },
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-31T07:19:26.445355Z",
     "iopub.status.busy": "2023-03-31T07:19:26.445013Z",
     "iopub.status.idle": "2023-03-31T07:19:26.571928Z",
     "shell.execute_reply": "2023-03-31T07:19:26.570103Z",
     "shell.execute_reply.started": "2023-03-31T07:19:26.445317Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../input/unnati-resume-2/unnati resume.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-9725f8b148df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/unnati-resume-2/unnati resume.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresume_text1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresume_text1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pdfminer/high_level.py\u001b[0m in \u001b[0;36mextract_text\u001b[0;34m(pdf_file, password, page_numbers, maxpages, caching, codec, laparams)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0mlaparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLAParams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput_string\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBinaryIO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# we opened in binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mrsrcmgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPDFResourceManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaching\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pdfminer/utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filename, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_handler\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAnyIO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIOBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/unnati-resume-2/unnati resume.pdf'"
     ]
    }
   ],
   "source": [
    "text = extract_text('../input/unnati-resume-2/unnati resume.pdf')\n",
    "resume_text1 = text.replace(\"\\n\", \" \")\n",
    "resume_text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-31T07:19:26.573136Z",
     "iopub.status.idle": "2023-03-31T07:19:26.573608Z"
    }
   },
   "outputs": [],
   "source": [
    "text = extract_text('../input/unnati-resume-2/unnati resume.pdf')\n",
    "resume_text1 = text.replace(\"\\n\", \" \")\n",
    "resume_text1\n",
    "text = extract_text('../input/shashank-resume/shashank Bajpai-Bca 2021 AUR - Shashank Bajpai.pdf')\n",
    "resume_text2 = text.replace(\"\\n\", \" \")\n",
    "resume_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-31T07:19:26.574810Z",
     "iopub.status.idle": "2023-03-31T07:19:26.575454Z"
    }
   },
   "outputs": [],
   "source": [
    "entities1 = predict(model, TOKENIZER, idx2tag, tag2idx, DEVICE, resume_text1)\n",
    "entities2 = predict(model, TOKENIZER, idx2tag, tag2idx, DEVICE, resume_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-31T07:19:26.576672Z",
     "iopub.status.idle": "2023-03-31T07:19:26.577423Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in entities1:\n",
    "    print(i['entity'], '-', i['text']) # abhi 4 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-03-31T07:19:26.579638Z",
     "iopub.status.idle": "2023-03-31T07:19:26.580465Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in entities2:\n",
    "    print(i['entity'], '-', i['text']) # abhi 4 epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
